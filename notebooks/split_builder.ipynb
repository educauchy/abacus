{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from stratification.params import SplitBuilderParams\n",
    "from stratification.split_builder import build_split, prepare_cat_data, assign_strata\n",
    "from prepilot.experiment_structures import BaseSplitElement\n",
    "\n",
    "\n",
    "class PrepilotSplitBuilder():\n",
    "    \"\"\"Columns with splits and injects will be added\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 spark,\n",
    "                 guests,\n",
    "                 group_sizes: List[int],\n",
    "                 stratification_params: SplitBuilderParams,\n",
    "                 iterations_number: int = 10):\n",
    "        \"\"\"There is class for calculation columns with injetcs and target/control splits\n",
    "\n",
    "        Args:\n",
    "            guests: dataframe with data for calculations injects and splits\n",
    "            metrics_names: list of metrics for which will be calculate injects columns\n",
    "            group_sizes: list of group sizes for split building\n",
    "            stratification_params: stratification parameters\n",
    "            iterations_number: number of columns that will be build for each group size\n",
    "\n",
    "        \"\"\"\n",
    "        self.guests = guests.withColumn(\"partion\", F.lit(1))\n",
    "        self.spark = spark\n",
    "        self.iterations_number = iterations_number\n",
    "        self.group_sizes = group_sizes\n",
    "        self.stratification_params = copy.deepcopy(stratification_params)\n",
    "        self.split_grid = self._build_splits_grid()\n",
    "\n",
    "    def _build_splits_grid(self):\n",
    "        return list(BaseSplitElement(el[0], el[1])\n",
    "                    for el in itertools.product(self.group_sizes, np.arange(1, self.iterations_number+1)))\n",
    "    \n",
    "\n",
    "    def apply_strata(self):\n",
    "\n",
    "        def strata_generator(df, stratification_params):\n",
    "            schema = (copy.deepcopy(df.schema)\n",
    "                    .add(T.StructField(\"strata\", T.StringType(), False))\n",
    "            )\n",
    "\n",
    "            @pandas_udf(schema, F.PandasUDFType.GROUPED_MAP)\n",
    "            def build_strata_pd(df: pd.DataFrame):\n",
    "\n",
    "                guests_data = prepare_cat_data(df, stratification_params)\n",
    "                guests_data_with_strata = assign_strata(guests_data.reset_index(drop=True), stratification_params)\n",
    "\n",
    "                return guests_data_with_strata\n",
    "\n",
    "            return build_strata_pd\n",
    "\n",
    "        strata = strata_generator(self.guests, self.stratification_params)\n",
    "        strata_result = self.guests.groupBy(\"partion\").apply(strata)\n",
    "\n",
    "        return strata_result\n",
    "\n",
    "\n",
    "    def build_split(self, \n",
    "                    guests_with_strata,\n",
    "                    split: BaseSplitElement):\n",
    "\n",
    "        def build_split_generator(stratification_params, \n",
    "                            split: BaseSplitElement):\n",
    "\n",
    "            schema = T.StructType([T.StructField(\"split_group_sizes\", T.StringType(), False),\n",
    "                                T.StructField(\"split_number\", T.IntegerType(), False),\n",
    "                                T.StructField(\"customer_rk\", T.LongType(), False),\n",
    "                                T.StructField(\"is_control\", T.IntegerType(), False)])\n",
    "\n",
    "            @pandas_udf(schema, F.PandasUDFType.GROUPED_MAP)\n",
    "            def build_split_pd(df: pd.DataFrame):\n",
    "                map_group_names_to_sizes={\n",
    "                    \"control\": split.control_group_size,\n",
    "                    \"target\": split.target_group_size\n",
    "                }\n",
    "\n",
    "                stratification_params.map_group_names_to_sizes = map_group_names_to_sizes\n",
    "\n",
    "                guests_groups = build_split(df, stratification_params)\n",
    "                guests_groups = guests_groups.join(\n",
    "                                pd.get_dummies(guests_groups[\"group_name\"])\n",
    "                                .add_prefix(\"is_\")\n",
    "                )\n",
    "                \n",
    "                guests_groups[\"split_group_sizes\"] = f\"{split.control_group_size}_{split.target_group_size}\"\n",
    "                guests_groups[\"split_number\"] = split.split_number\n",
    "                            \n",
    "                result = pd.DataFrame({\"split_group_sizes\": guests_groups[\"split_group_sizes\"],\n",
    "                                       \"split_number\": guests_groups[\"split_number\"],\n",
    "                                       \"customer_rk\": guests_groups[\"customer_rk\"],\n",
    "                                       \"is_control\": guests_groups[\"is_control\"]\n",
    "                                     })\n",
    "\n",
    "                return result\n",
    "            return build_split_pd\n",
    "\n",
    "        split = build_split_generator(self.stratification_params,\n",
    "                                      split)\n",
    "        split_result = guests_with_strata.groupBy(\"partion\").apply(split)\n",
    "\n",
    "        return split_result\n",
    "\n",
    "    def collect(self):\n",
    "        \"\"\"Calculate multiple split with stratification\n",
    "\n",
    "        Returns: DataFrame with split column\n",
    "\n",
    "        \"\"\"\n",
    "        schema = T.StructType([T.StructField(\"split_group_sizes\", T.StringType(), False),\n",
    "                            T.StructField(\"split_number\", T.IntegerType(), False),\n",
    "                            T.StructField(\"customer_rk\", T.LongType(), False),\n",
    "                            T.StructField(\"is_control\", T.IntegerType(), False)])\n",
    "\n",
    "        result = self.spark.createDataFrame([], schema)\n",
    "        data_with_strata = self.apply_strata()\n",
    "\n",
    "        for split_param in self.split_grid:\n",
    "            split_data = self.build_split(data_with_strata, split_param)\n",
    "            result = result.unionAll(split_data)\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from prepilot.params import PrepilotParams\n",
    "\n",
    "datestart=datetime.date(2021, 4, 20)\n",
    "datepostperiod=datetime.date(2021, 4, 28)\n",
    "prepilot_params = PrepilotParams(\n",
    "    datestart=datestart,\n",
    "    datepostperiod=datepostperiod,\n",
    "    metrics_names=['rto_novat'],\n",
    "    injects=[1.01, 1.02, 1.03, 1.04, 1.05, 1.06, 1.07, 1.08, 1.09, 1.1,1.15,1.2,1.25],\n",
    "    min_group_size=1000, \n",
    "    max_group_size=3000, \n",
    "    step=1000,\n",
    "    iterations_number = 3,\n",
    "    max_beta_score=0.2,\n",
    "    min_beta_score=0.05,\n",
    "    experiment_plu_codes=[''],\n",
    "    experiment_synth_catalog_ids=[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_builder_params = SplitBuilderParams(\n",
    "    map_group_names_to_sizes={\n",
    "        'control': 10000,\n",
    "        'target': 10000\n",
    "    },\n",
    "    cols=[\n",
    "        'region',\n",
    "        'rto',\n",
    "        #'fm',\n",
    "        #'points_balance',\n",
    "        #'exp_points_balance',\n",
    "        #'offer_rk_goal',\n",
    "        #'offer_rk_campaign',\n",
    "        #'rto_synthetic_cat',\n",
    "        #'fm_synthetic_cat',\n",
    "        #'count_checks_synthetic_cat'\n",
    "    ],\n",
    "    cat_cols=[\n",
    "        #'offer_rk_goal',\n",
    "        #'offer_rk_campaign'\n",
    "    ],\n",
    "    pvalue=0.05,\n",
    "    n_top_cat=100,\n",
    "    stat_test=\"ttest_ind\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/19 12:40:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import copy\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
    "\n",
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(\"local\")\n",
    "         .appName(\"gbc_ab_pyspark\")\n",
    "         # Add postgres jar\n",
    "         #.config(\"spark.driver.extraClassPath\", \"/home/jovyan/work/jars/postgresql-9.4.1207.jar\")\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\",True)\n",
    "    .option(\"sep\", \";\")\n",
    "    .load(\"TLO.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df.withColumnRenamed(\"customer_id\",\"customer_rk\")\n",
    "        .withColumnRenamed(\"moda_city\",\"region\")\n",
    "        .withColumnRenamed(\"post_count_orders\",\"rto\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepilot_guests_collector = PrepilotSplitBuilder(spark, df,\n",
    "                                                [(10000,10000), (20000,20000),(30000,30000)],\n",
    "                                                split_builder_params,\n",
    "                                                5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egorshishkovets/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/pandas/group_ops.py:81: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "splited_df = prepilot_guests_collector.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/egorshishkovets/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/egorshishkovets/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Users/egorshishkovets/opt/anaconda3/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7r/n_prgqpn7898pkrs_50zgvmw0000gn/T/ipykernel_39369/1259518797.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msplited_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "splited_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from stratification.params import SplitBuilderParams\n",
    "from prepilot.abstract_experiment_builder import AbstractExperimentBuilder\n",
    "from prepilot.params import PrepilotParams\n",
    "from post_analysis.stat_test import PeriodStatTest\n",
    "\n",
    "\n",
    "_ERROR_RES_SCHEMA = T.StructType([\n",
    "                    T.StructField(\"group_sizes\", T.StringType(), False),\n",
    "                    T.StructField(\"metric\", T.StringType(), False),\n",
    "                    T.StructField(\"MDE\", T.FloatType(), False),\n",
    "                    T.StructField(\"is_effect_found\", T.IntegerType(), False)\n",
    "                    ])\n",
    "\n",
    "class PrepilotExperimentBuilder(AbstractExperimentBuilder):\n",
    "    \"\"\"Calculates I and II type errors for different group sizes and injects\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 spark,\n",
    "                 guests: DataFrame,\n",
    "                 experiment_params: PrepilotParams):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            guests: dataframe that collected by PrepilotGuestsCollector\n",
    "            experiment_params: prameters for prepilot experiments\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(spark, \n",
    "                         guests,\n",
    "                         experiment_params)\n",
    "        self._number_of_decimals = 10\n",
    "\n",
    "    def _calc_experiment(self,\n",
    "                         guests_with_splits,\n",
    "                         res_schema,\n",
    "                         metric_name,\n",
    "                         inject=1.0):\n",
    "\n",
    "        def calc_split_experiment_generator(stat_test_params, \n",
    "                                            metric_name, \n",
    "                                            inject=1.0):\n",
    "\n",
    "            @pandas_udf(res_schema, F.PandasUDFType.GROUPED_MAP)\n",
    "            def calc_split_experiment_pd(guests_with_splits):\n",
    "                \"\"\"Calculates stat test for one experiment grid element\n",
    "\n",
    "                Args:\n",
    "                    guests_with_splits: dataframe with calculated splits for experiment\n",
    "\n",
    "                Returns: pandas DataFrame with calculated stat test and experiment parameters\n",
    "\n",
    "                \"\"\"\n",
    "                group_sizes = guests_with_splits.split_group_sizes.values[0]\n",
    "                control = guests_with_splits[guests_with_splits[metric_name] == 0][metric_name].values\n",
    "                target = guests_with_splits[guests_with_splits[metric_name] == 1][metric_name].values * inject\n",
    "                stat_test = PeriodStatTest(target, control, \"\", stat_test_params)\n",
    "                stat_result = stat_test.calculate_period_effect()\n",
    "                return pd.DataFrame({\"group_sizes\": [group_sizes],\n",
    "                                    \"metric\": [metric_name],\n",
    "                                    \"MDE\": [inject],\n",
    "                                    \"is_effect_found\": stat_result[\"effect__significance\"]\n",
    "                                    })\n",
    "\n",
    "            return calc_split_experiment_pd\n",
    "\n",
    "        calculate_experiment = calc_split_experiment_generator(self.stat_test_params, metric_name, inject)\n",
    "        result_df = guests_with_splits.groupBy([\"split_group_sizes\", \"split_number\" ]).apply(calculate_experiment)\n",
    "        return result_df    \n",
    "\n",
    "    def calc_alpha_error(self,\n",
    "                         guests_with_splits,\n",
    "                         res_schema\n",
    "                         ):\n",
    "\n",
    "        alpha_error_result = self.spark.createDataFrame([], res_schema)\n",
    "\n",
    "        for metric_name in self._experiment_params.metrics_names:\n",
    "            alpha_error_df = self._calc_experiment(guests_with_splits,\n",
    "                                                   res_schema, \n",
    "                                                   metric_name)\n",
    "            alpha_error_result = alpha_error_result.unionAll(alpha_error_df)\n",
    "        \n",
    "        alpha_agg = (alpha_error_result\n",
    "            .groupBy([\"metric\", \"group_sizes\"])\n",
    "            .agg(F.avg(\"is_effect_found\").alias(\"alpha_error\"))\n",
    "        )\n",
    "        return alpha_agg.groupBy(\"metric\").pivot(\"group_sizes\").sum(\"alpha_error\")\n",
    "\n",
    "    def calc_beta_error(self,\n",
    "                        guests_with_splits,\n",
    "                        res_schema\n",
    "                        ):\n",
    "\n",
    "        beta_error_result = self.spark.createDataFrame([], res_schema)\n",
    "        for inject in self._experiment_params.injects:\n",
    "            for metric_name in self._experiment_params.metrics_names:\n",
    "                beta_error_df = self._calc_experiment(guests_with_splits,\n",
    "                                                      res_schema,\n",
    "                                                      metric_name,\n",
    "                                                      inject)\n",
    "                beta_error_result = beta_error_result.unionAll(beta_error_df)\n",
    "        \n",
    "        beta_agg = (beta_error_result\n",
    "            .groupBy([\"metric\", \"group_sizes\", \"MDE\"])\n",
    "            .agg(1.0 - F.avg(\"is_effect_found\").alias(\"beta\"))\n",
    "        )\n",
    "\n",
    "        return beta_agg.groupBy([\"metric\", \"MDE\"]).pivot(\"group_sizes\").sum(\"beta\")\n",
    "\n",
    "\n",
    "    def collect(self, stratification_params: SplitBuilderParams) -> pd.DataFrame:\n",
    "        \"\"\"Calculates I and II types error using prepilot parameters.\n",
    "\n",
    "        Args:\n",
    "            stratification_params: params for stratification\n",
    "            full: if True function will return full dataframe with results.\n",
    "            Otherwise will be returned only max calculated MDE for each size.\n",
    "\n",
    "        Returns: pandas DataFrames with aggregated results of experiment.\n",
    "\n",
    "        \"\"\"\n",
    "        prepilot_split_builder = PrepilotSplitBuilder(self.spark, \n",
    "                                                        self.guests,\n",
    "                                                        self.group_sizes,\n",
    "                                                        stratification_params,\n",
    "                                                        3)#self._experiment_params.iterations_number)\n",
    "\n",
    "        prepilot_guests = prepilot_split_builder.collect()\n",
    "        prepilot_guests = (prepilot_guests.join(self.guests, \n",
    "                                                on=\"customer_rk\", \n",
    "                                                how=\"inner\")\n",
    "                        .select(prepilot_guests.columns+self._experiment_params.metrics_names)\n",
    "        )\n",
    "\n",
    "        beta = self.calc_beta_error(prepilot_guests, _ERROR_RES_SCHEMA)\n",
    "        alpha = self.calc_alpha_error(prepilot_guests, _ERROR_RES_SCHEMA)\n",
    "        return beta, alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datestart=datetime.date(2021, 4, 20)\n",
    "datepostperiod=datetime.date(2021, 4, 28)\n",
    "prepilot_params = PrepilotParams(\n",
    "    datestart=datestart,\n",
    "    datepostperiod=datepostperiod,\n",
    "    metrics_names=['rto'],\n",
    "    injects=[1.01, 1.02, 1.03],\n",
    "    min_group_size=10000, \n",
    "    max_group_size=30000, \n",
    "    step=10000,\n",
    "    iterations_number = 30,\n",
    "    max_beta_score=0.2,\n",
    "    min_beta_score=0.05,\n",
    "    experiment_plu_codes=[''],\n",
    "    experiment_synth_catalog_ids=[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepilot = PrepilotExperimentBuilder(spark, df,\n",
    "                                     prepilot_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "beta, alpha = prepilot.collect(split_builder_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:=====================================================>(592 + 1) / 601]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+-----------+-----------+\n",
      "|metric| MDE|10000_10000|20000_20000|30000_30000|\n",
      "+------+----+-----------+-----------+-----------+\n",
      "|   rto|1.01|        1.0|        1.0|        1.0|\n",
      "|   rto|1.02|        1.0|        1.0|        1.0|\n",
      "|   rto|1.03|        1.0|        1.0|        1.0|\n",
      "+------+----+-----------+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "beta.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 93:================================================>     (181 + 1) / 201]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+-----------+\n",
      "|metric|10000_10000|20000_20000|30000_30000|\n",
      "+------+-----------+-----------+-----------+\n",
      "|   rto|        1.0|        1.0|        1.0|\n",
      "+------+-----------+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "alpha.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+---+---------------+\n",
      "|group_sizes|metric|MDE|is_effect_found|\n",
      "+-----------+------+---+---------------+\n",
      "|10000_10000|   rto|1.0|              1|\n",
      "+-----------+------+---+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "beta.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (splited_df.groupBy([\"split_group_sizes\"])\n",
    "            .agg(1.0 - F.avg(\"is_control\").alias(\"beta\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 112:==================================================>(2968 + 1) / 3001]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------------------+\n",
      "|split_group_sizes|(1.0 - avg(is_control) AS beta)|\n",
      "+-----------------+-------------------------------+\n",
      "|      10000_10000|            0.49992495121829184|\n",
      "|      20000_20000|                            0.5|\n",
      "|      30000_30000|             0.5000083320835208|\n",
      "+-----------------+-------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "t.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = (splited_df.groupBy([\"split_group_sizes\"])\n",
    "            .agg(F.avg(\"is_control\").alias(\"beta\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 122:==================================================>(2986 + 1) / 3001]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|split_group_sizes|              beta|\n",
      "+-----------------+------------------+\n",
      "|      10000_10000|0.5000750487817082|\n",
      "|      20000_20000|               0.5|\n",
      "|      30000_30000|0.4999916679164792|\n",
      "+-----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "t1.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "626acd8304246c39c892b04a088b1f86fa4d1e9c8375290e6951e511351ad7f7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
